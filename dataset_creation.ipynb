{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd03e9b354ce98f47c9dd0b59702fdeb863bfffe347fccce27b4aea934dfd1532aa",
   "display_name": "Python 3.7.10 64-bit ('sentiment-analysis': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5269e6b9e2975eb3b122cde3fd2cf669a9b5e3fde7c7f789945efb52d7fd6e83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import nltk\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import string\n",
    "from liwc import Liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Diego\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"pt_core_news_sm\")\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "nltk.download('punkt')\n",
    "#classes = ['VERB', 'NOUN', 'ADJ']\n",
    "LIWCLocation = 'LIWC2015.dic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value\n",
    "def get_key(classes, my_dict):\n",
    "    keys = []\n",
    "    for _class in classes:\n",
    "        for key, value in my_dict.items():\n",
    "            if _class == value:\n",
    "                keys.append(key)\n",
    "    \n",
    "    return \" \".join(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_tagger(text):\n",
    "#     doc = nlp(text)\n",
    "#     pos_tagging_dict = {}\n",
    "#     for token in doc:\n",
    "#         pos_tagging_dict[token.text] = token.pos_\n",
    "#     return pos_tagging_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemma_list = []\n",
    "#     for token in doc:\n",
    "#         lemma_list.append(token.lemma_)\n",
    "#     return \" \".join(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=\"portuguese\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"pt\"]]\n",
    "    text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    pattern = r\"@[a-zA-Z0-9]+\"\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    pattern = r\"#[a-zA-Z0-9]+\"\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(text):\n",
    "    text = re.sub(r\"\"\"['\"]+\"\"\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liwc_analysis(text):\n",
    "    liwc = Liwc(LIWCLocation)\n",
    "    tokens = tokenize(text)\n",
    "    liwc_analysis = dict(liwc.parse(tokens))\n",
    "    return liwc_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subjectivity(liwc_analysis):    \n",
    "    classes = ['compare (Comparisons)', \n",
    "               'affect (Affect)',\n",
    "               'posemo (Positive Emotions)', \n",
    "               'negemo (Negative Emotions)']\n",
    "    for class_ in classes:\n",
    "        if class_ in liwc_analysis:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentiment_words_rate(text):\n",
    "#     senti_dict = {\"posemo\": 0,\n",
    "#                   \"negemo\": 0}\n",
    "#     tokens = nltk.tokenize.word_tokenize(text, language=\"portuguese\")\n",
    "#     if len(tokens) > 0:\n",
    "#         for token in tokens:\n",
    "#             if token in liwc:\n",
    "#                 if \"posemo\" in liwc[token]:\n",
    "#                     senti_dict[\"posemo\"] += 1\n",
    "#                 elif \"negemo\" in liwc[token]:\n",
    "#                     senti_dict[\"negemo\"] += 1\n",
    "#         senti_dict[\"posemo\"] = round(senti_dict[\"posemo\"] / len(tokens), 2)\n",
    "#         senti_dict[\"negemo\"] = round(senti_dict[\"negemo\"] / len(tokens), 2)\n",
    "#     return senti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalizer = Normaliser()\n",
    "    text = normalizer.normalise(text)\n",
    "    #text = text.replace('username', '')\n",
    "    #text = text.replace('hashtag', '')\n",
    "    #text = text.replace('number', '')\n",
    "    #text = text.replace('url', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['preprocessed_text'] = df.text.apply(remove_urls)\n",
    "    df['preprocessed_text'] = df.preprocessed_text.apply(remove_tags)\n",
    "    df['preprocessed_text'] = df.preprocessed_text.apply(remove_hashtags)\n",
    "    df['preprocessed_text'] = df.preprocessed_text.apply(remove_emoji)\n",
    "    df['preprocessed_text'] = df.preprocessed_text.apply(remove_quotes)\n",
    "\n",
    "    custom_pipeline = [preprocessing.fillna,\n",
    "                       preprocessing.lowercase,\n",
    "                       preprocessing.remove_brackets,\n",
    "                       preprocessing.remove_digits,\n",
    "                       preprocessing.remove_punctuation,\n",
    "                       preprocessing.remove_whitespace]\n",
    "    df['preprocessed_text'] = df.preprocessed_text.pipe(hero.clean, custom_pipeline)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns_from_liwc(df):\n",
    "    classes = ['compare (Comparisons)',\n",
    "                'affect (Affect)',\n",
    "                'posemo (Positive Emotions)',\n",
    "                'negemo (Negative Emotions)']\n",
    "    df['liwc_analysis'] = df['liwc_analysis'].apply(lambda x: ast.literal_eval(x))\n",
    "    for col in classes:\n",
    "        df[col] = df.liwc_analysis.apply(lambda x: x[col] if col in x.keys() else '0')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    con = sqlite3.connect(\"tweets.db\")\n",
    "    sql = \"select id_str, text from tweets where text not like '%RT @%' order by random() limit 2000\"\n",
    "    df = pd.read_sql(sql, con=con, index_col=\"id_str\")\n",
    "    df = preprocess(df)\n",
    "    df['liwc_analysis'] = df.preprocessed_text.apply(get_liwc_analysis)\n",
    "    df['is_subjective'] = df.liwc_analysis.apply(check_subjectivity)\n",
    "    #df = filter_dataset_with_sentiment_words(df)\n",
    "    return df"
   ]
  }
 ]
}